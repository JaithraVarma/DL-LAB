{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "FNN"
      ],
      "metadata": {
        "id": "gC0SB9OFKD37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torchvision # torch package for vision related things\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "import torchvision.datasets as datasets  # Standard datasets\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "\n",
        "# Here we create our simple neural network. For more details here we are subclassing and\n",
        "# inheriting from nn.Module, this is the most general way to create your networks and\n",
        "# allows for more flexibility. I encourage you to also check out nn.Sequential which\n",
        "# would be easier to use in this scenario but I wanted to show you something that\n",
        "# \"always\" works and is a general approach.\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        \"\"\"\n",
        "        Here we define the layers of the network. We create two fully connected layers\n",
        "        Parameters:\n",
        "            input_size: the size of the input, in this case 784 (28x28)\n",
        "            num_classes: the number of classes we want to predict, in this case 10 (0-9)\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(NN, self).__init__()\n",
        "        # Our first linear layer take input_size, in this case 784 nodes to 50\n",
        "        # and our second linear layer takes 50 to the num_classes we have, in\n",
        "        # this case 10.\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.fc2 = nn.Linear(50, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x here is the mnist images and we run it through fc1, fc2 that we created above.\n",
        "        we also add a ReLU activation function in between and for that (since it has no parameters)\n",
        "        I recommend using nn.functional (F)\n",
        "        Parameters:\n",
        "            x: mnist images\n",
        "        Returns:\n",
        "            out: the output of the network\n",
        "        \"\"\"\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "\n",
        "# Load Data\n",
        "train_dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize network\n",
        "model = NN(input_size=input_size, num_classes=num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # Get to correct shape\n",
        "        data = data.reshape(data.shape[0], -1)\n",
        "\n",
        "        # Forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# Check accuracy on training & test to see how good our model\n",
        "def check_accuracy(loader, model):\n",
        "    \"\"\"\n",
        "    Check accuracy of our trained model given a loader and a model\n",
        "    Parameters:\n",
        "        loader: torch.utils.data.DataLoader\n",
        "            A loader for the dataset you want to check accuracy on\n",
        "        model: nn.Module\n",
        "            The model you want to check accuracy on\n",
        "    Returns:\n",
        "        acc: float\n",
        "            The accuracy of the model on the dataset given by the loader\n",
        "    \"\"\"\n",
        "\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n",
        "    with torch.no_grad():\n",
        "        # Loop through the data\n",
        "        for x, y in loader:\n",
        "\n",
        "            # Move data to device\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            # Get to correct shape\n",
        "            x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "            # Forward pass\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "\n",
        "            # Check how many we got correct\n",
        "            num_correct += (predictions == y).sum()\n",
        "\n",
        "            # Keep track of number of samples\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    model.train()\n",
        "    return num_correct/num_samples\n",
        "\n",
        "# Check accuracy on training & test to see how good our model\n",
        "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n",
        "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
      ],
      "metadata": {
        "id": "6XXnX0iCKF6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "VOr5EmjCKJdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A simple walkthrough of how to code a convolutional neural network (CNN)\n",
        "using the PyTorch library. For demonstration we train it on the very\n",
        "common MNIST dataset of handwritten digits. In this code we go through\n",
        "how to create the network as well as initialize a loss function, optimizer,\n",
        "check accuracy and more.\n",
        "Programmed by Aladdin Persson\n",
        "* 2020-04-08: Initial coding\n",
        "* 2021-03-24: More detailed comments and small revision of the code\n",
        "\"\"\"\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torchvision # torch package for vision related things\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "import torchvision.datasets as datasets  # Standard datasets\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "\n",
        "# Simple CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=8,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1),\n",
        "        )\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=8,\n",
        "            out_channels=16,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "in_channels = 1\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "\n",
        "# Load Data\n",
        "train_dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize network\n",
        "model = CNN(in_channels=in_channels, num_classes=num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "# Check accuracy on training & test to see how good our model\n",
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    return num_correct/num_samples\n",
        "\n",
        "\n",
        "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n",
        "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
      ],
      "metadata": {
        "id": "D3mnbl4dKLBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save and Load**"
      ],
      "metadata": {
        "id": "FaflrhYWKSL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Small code example of how to save and load checkpoint of a model.\n",
        "This example doesn't perform any training, so it would be quite useless.\n",
        "In practice you would save the model as you train, and then load before \n",
        "continuining training at another point.\n",
        "Video explanation of code & how to save and load model: https://youtu.be/g6kQl_EFn84\n",
        "Got any questions leave a comment on youtube :)\n",
        "Coded by Aladdin Persson <aladdin dot person at hotmail dot com>\n",
        "-   2020-04-07 Initial programming\n",
        "\"\"\"\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torch.nn.functional as F  # All functions that don't have any parameters\n",
        "from torch.utils.data import (\n",
        "    DataLoader,\n",
        ")  # Gives easier dataset managment and creates mini batches\n",
        "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Initialize network\n",
        "    model = torchvision.models.vgg16(pretrained=False)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
        "    # Try save checkpoint\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "    # Try load checkpoint\n",
        "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Rf1f9pSuKjLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer Learning**"
      ],
      "metadata": {
        "id": "41NpcimZKmoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torch.nn.functional as F  # All functions that don't have any parameters\n",
        "from torch.utils.data import (\n",
        "    DataLoader,\n",
        ")  # Gives easier dataset managment and creates mini batches\n",
        "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "num_classes = 10\n",
        "learning_rate = 1e-3\n",
        "batch_size = 1024\n",
        "num_epochs = 5\n",
        "\n",
        "# Simple Identity class that let's input pass without changes\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load pretrain model & modify it\n",
        "model = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "# If you want to do finetuning then set requires_grad = False\n",
        "# Remove these two lines if you want to train entire model,\n",
        "# and only want to load the pretrain weights.\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.avgpool = Identity()\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(512, 100), nn.ReLU(), nn.Linear(100, num_classes)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Load Data\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
        ")\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses):.5f}\")\n",
        "\n",
        "# Check accuracy on training & test to see how good our model\n",
        "\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "    if loader.dataset.train:\n",
        "        print(\"Checking accuracy on training data\")\n",
        "    else:\n",
        "        print(\"Checking accuracy on test data\")\n",
        "\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(\n",
        "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n",
        "        )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "check_accuracy(train_loader, model)"
      ],
      "metadata": {
        "id": "wTb8KRYVKqaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**custom dataset**"
      ],
      "metadata": {
        "id": "qCu5W2zZKvR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
        "import torchvision\n",
        "import os\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader,\n",
        ")  # Gives easier dataset managment and creates mini batches\n",
        "\n",
        "\n",
        "class CatsAndDogsDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
        "        image = io.imread(img_path)\n",
        "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (image, y_label)\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "in_channel = 3\n",
        "num_classes = 2\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# Load Data\n",
        "dataset = CatsAndDogsDataset(\n",
        "    csv_file=\"cats_dogs.csv\",\n",
        "    root_dir=\"cats_dogs_resized\",\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "# Dataset is actually a lot larger ~25k images, just took out 10 pictures\n",
        "# to upload to Github. It's enough to understand the structure and scale\n",
        "# if you got more images.\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [5, 5])\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model\n",
        "model = torchvision.models.googlenet(pretrained=True)\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")\n",
        "\n",
        "# Check accuracy on training to see how good our model is\n",
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(\n",
        "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n",
        "        )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "print(\"Checking accuracy on Training Set\")\n",
        "check_accuracy(train_loader, model)\n",
        "\n",
        "print(\"Checking accuracy on Test Set\")\n",
        "check_accuracy(test_loader, model)"
      ],
      "metadata": {
        "id": "85K0fiedKz7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}